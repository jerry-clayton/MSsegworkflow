{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6a949f7d-e292-40cd-a1a4-c74eb2657ec5",
   "metadata": {},
   "source": [
    "# Mean Shift Segmentation Batch Runner\n",
    "\n",
    "Author: [Jerry Clayton](https://github.com/jerry-clayton)\n",
    "\n",
    "For: [Ni-Meister Lab](http://www.geography.hunter.cuny.edu/~wenge/)\n",
    "\n",
    "Adapted from [Ian Grant's Script](https://github.com/i-c-grant/ni-meister-gedi-biomass/blob/main/run_on_maap.py)\n",
    "\n",
    "#### This Jupyter notebook handles the batch processing of the modified Mean Shift tree segmentation workflow developed by the author and Dr. Ni-Meister on the NASA MAAP platform. \n",
    "\n",
    "#### The [AMS3D](https://www.sciencedirect.com/science/article/abs/pii/S0034425716302292) was first proposed by Ferraz et. al, and this implementation depends on Dr. Nikolai Knapp's [MeanShiftR](https://github.com/niknap/MeanShiftR/tree/master/R) package\n",
    "\n",
    "#### This workflow is executed in four parts: Split, Segment, Reconcile, and Merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8d2fb8ae-9e82-4abb-8c1f-0dd8144f88e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import logging\n",
    "import os\n",
    "import shutil\n",
    "import time\n",
    "import glob\n",
    "import tarfile\n",
    "\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "from typing import Dict, List\n",
    "\n",
    "import click\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from geopandas import GeoDataFrame\n",
    "from maap.maap import MAAP\n",
    "from maap.Result import Granule\n",
    "\n",
    "maap = MAAP(maap_host='api.maap-project.org')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0356ca5d-ddf5-41f7-97df-ba30649cc12d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_file_url(filename):\n",
    "    url_first_part = \"s3://maap-ops-workspace/jclayton0/sq_km_tiles_norm/\" \n",
    "    url = f'{url_first_part}{filename}'\n",
    "    return url\n",
    "\n",
    "def build_test_file_url(filename):\n",
    "    url_first_part = \"s3://maap-ops-workspace/jclayton0/test-input-sm-tiles/\" \n",
    "    url = f'{url_first_part}{filename}'\n",
    "    return url\n",
    "\n",
    "def get_split_kwargs(fileurl):\n",
    "     job_kwargs = {\n",
    "            \"identifier\": \"Mean Shift Split LAS\",\n",
    "            \"algo_id\": \"MS-Step-1-Split\",\n",
    "            \"version\": \"main\",\n",
    "            \"username\": \"jclayton0\",\n",
    "            \"queue\": \"maap-dps-worker-64gb\",\n",
    "            \"LAS\": fileurl,\n",
    "            \"Subplot_width\": 25,\n",
    "            \"Buffer_width\": 10\n",
    "    }\n",
    "    \n",
    "     return job_kwargs\n",
    "\n",
    "def get_segment_kwargs(fileurl):\n",
    "    job_kwargs = {\n",
    "            \"identifier\": \"Mean Shift Segment LAS\",\n",
    "            \"algo_id\": \"MS-Step-2-Segment-v2\",\n",
    "            \"version\": \"main\",\n",
    "            \"username\": \"jclayton0\",\n",
    "            \"queue\": \"maap-dps-worker-64gb\",\n",
    "            \"Point Cloud RDS\": fileurl,\n",
    "            \"Subplot_widthFrac_cores\": 0.9\n",
    "    }\n",
    "    \n",
    "    return job_kwargs\n",
    "\n",
    "def get_reconcile_kwargs(tarball_url, las_url):\n",
    "    job_kwargs = {\n",
    "            \"identifier\": \"Mean Shift Reconcile LAS\",\n",
    "            \"algo_id\": \"MS-Step-3-Reconcile-v3\",\n",
    "            \"version\": \"main\",\n",
    "            \"username\": \"jclayton0\",\n",
    "            \"queue\": \"maap-dps-worker-64gb\",\n",
    "            \"tarball\": tarball_url,\n",
    "            \"original_las\": las_url\n",
    "    }\n",
    "    \n",
    "    return job_kwargs\n",
    "\n",
    "def get_merge_kwargs(fileurl):\n",
    "    job_kwargs = {\n",
    "            \"identifier\": \"Mean Shift Merge Trees\",\n",
    "            \"algo_id\": \"MS-Step-4-Merge\",\n",
    "            \"version\": \"main\",\n",
    "            \"username\": \"jclayton0\",\n",
    "            \"queue\": \"maap-dps-worker-64gb\",\n",
    "            \"segmented_las\": fileurl\n",
    "    }\n",
    "    \n",
    "    return job_kwargs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cb2cc333-4aa5-475c-9c78-fc97e526dde9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def local_url_to_s3(url):\n",
    "    second = str.split(url,\"my-private-bucket\")[1]\n",
    "    full = f\"s3://maap-ops-workspace/jclayton0{second}\"\n",
    "    return full\n",
    "\n",
    "def get_old_jobs_list(old_jobs_path):\n",
    "    \n",
    "    with open(old_jobs_path, 'r') as file:\n",
    "        old_jobs = file.readlines()\n",
    "    \n",
    "    old_jobs = [line.strip() for line in old_jobs]\n",
    "    return old_jobs\n",
    "\n",
    "def get_succeeded_jobs_in_list(job_ids):\n",
    "    \n",
    "    succeeded_job_ids = [job_id for job_id in job_ids\n",
    "                         if job_status_for(job_id) == \"Succeeded\"]\n",
    "    return succeeded_job_ids\n",
    "\n",
    "def get_failed_jobs_in_list(job_ids):\n",
    "    \n",
    "    failed_job_ids = [job_id for job_id in job_ids\n",
    "                      if job_status_for(job_id) == \"Failed\"]\n",
    "    return failed_job_ids\n",
    "\n",
    "def get_other_status_jobs_in_list(job_ids):\n",
    "\n",
    "    other_job_ids = [job_id for job_id in job_ids\n",
    "                     if job_status_for(job_id)\n",
    "                     not in [\"Succeeded\", \"Failed\"]]\n",
    "    return other_job_ids\n",
    "\n",
    "\n",
    "import json\n",
    "def get_failed_job_input_LAS(failed_json_path):\n",
    "\n",
    "    with open(failed_json_path,'r') as file:\n",
    "        data = json.load(file)\n",
    "        \n",
    "    return data.get('params').get('job_specification').get('params')[0].get('value')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "62bab460-0b22-433b-a63d-237f66fd9915",
   "metadata": {},
   "outputs": [],
   "source": [
    "def job_status_for(job_id: str) -> str:\n",
    "    return maap.getJobStatus(job_id)\n",
    "\n",
    "def job_result_for(job_id: str) -> str:\n",
    "    return maap.getJobResult(job_id)[0]\n",
    "\n",
    "def to_job_output_dir(job_result_url: str, username: str) -> str:\n",
    "    return (f\"/projects/my-private-bucket/\"\n",
    "            f\"{job_result_url.split(f'/{username}/')[1]}\")\n",
    "\n",
    "def to_failed_job_params(job_result_url: str) -> str:\n",
    "    return (f\"../triaged-jobs/\"\n",
    "            f\"{job_result_url.split(f'/triaged_job/')[1]}/_job.json\")\n",
    "\n",
    "\n",
    "def log_and_print(message: str):\n",
    "    logging.info(message)\n",
    "    click.echo(message)\n",
    "\n",
    "def update_job_states(job_states: Dict[str, str],\n",
    "                      final_states: List[str],\n",
    "                      batch_size: int,\n",
    "                      delay: int) -> Dict[str, str]:\n",
    "    \"\"\"Update the job states dictionary in place.\n",
    "\n",
    "    Updating occurs in batches, with a delay in seconds between batches.\n",
    "\n",
    "    Return the number of jobs updated to final states.\n",
    "    \"\"\"\n",
    "    batch_count = 0\n",
    "    n_updated_to_final = 0\n",
    "    for job_id, state in job_states.items():\n",
    "        if state not in final_states:\n",
    "            new_state: str = job_status_for(job_id)\n",
    "            job_states[job_id] = new_state\n",
    "            if new_state in final_states:\n",
    "                n_updated_to_final += 1\n",
    "            batch_count += 1\n",
    "        # Sleep after each batch to avoid overwhelming the API\n",
    "        if batch_count == batch_size:\n",
    "            time.sleep(delay)\n",
    "            batch_count = 0\n",
    "\n",
    "    return n_updated_to_final\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2039bda1-24d5-4525-a345-c042c8c37e0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'message': 'Not authorized.'}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "maap.getQueues().json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6da058da-7ef8-4c93-9b8a-f326fffbbc02",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Source Dir for large tiles, relative path\n",
    "long_list = glob.glob('../my-private-bucket/sq_km_tiles_norm/*')\n",
    "test_list = glob.glob('../my-private-bucket/test-input-sm-tiles/*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bf8929f8-f940-4195-bc3d-70da7a85f3d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "files = list()\n",
    "for file in long_list:\n",
    "    files.append(str.split(file, '/')[3])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4bf88499-208e-48c7-84ba-3cf62abd8df7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['TEAK_large_001.las',\n",
       " 'TEAK_large_002.las',\n",
       " 'TEAK_large_003.las',\n",
       " 'TEAK_large_004.las',\n",
       " 'TEAK_large_005.las',\n",
       " 'TEAK_large_006.las',\n",
       " 'TEAK_large_007.las',\n",
       " 'TEAK_large_008.las',\n",
       " 'TEAK_large_009.las',\n",
       " 'TEAK_large_010.las',\n",
       " 'TEAK_large_011.las',\n",
       " 'TEAK_large_012.las',\n",
       " 'TEAK_large_013.las',\n",
       " 'TEAK_large_014.las',\n",
       " 'TEAK_large_015.las',\n",
       " 'TEAK_large_016.las',\n",
       " 'TEAK_large_017.las',\n",
       " 'TEAK_large_018.las',\n",
       " 'TEAK_large_019.las',\n",
       " 'TEAK_large_020.las',\n",
       " 'TEAK_large_021.las',\n",
       " 'TEAK_large_022.las',\n",
       " 'TEAK_large_023.las',\n",
       " 'TEAK_large_024.las',\n",
       " 'TEAK_large_025.las',\n",
       " 'TEAK_large_026.las',\n",
       " 'TEAK_large_027.las',\n",
       " 'TEAK_large_028.las',\n",
       " 'TEAK_large_029.las',\n",
       " 'TEAK_large_030.las',\n",
       " 'TEAK_large_031.las',\n",
       " 'TEAK_large_032.las',\n",
       " 'TEAK_large_033.las',\n",
       " 'TEAK_large_034.las',\n",
       " 'TEAK_large_035.las',\n",
       " 'TEAK_large_036.las',\n",
       " 'TEAK_large_037.las',\n",
       " 'TEAK_large_038.las',\n",
       " 'TEAK_large_039.las',\n",
       " 'TEAK_large_040.las',\n",
       " 'TEAK_large_041.las',\n",
       " 'TEAK_large_042.las',\n",
       " 'TEAK_large_043.las',\n",
       " 'TEAK_large_044.las',\n",
       " 'TEAK_large_045.las',\n",
       " 'TEAK_large_046.las',\n",
       " 'TEAK_large_047.las',\n",
       " 'TEAK_large_048.las',\n",
       " 'TEAK_large_049.las',\n",
       " 'TEAK_large_050.las',\n",
       " 'TEAK_large_051.las',\n",
       " 'TEAK_large_052.las',\n",
       " 'TEAK_large_053.las',\n",
       " 'TEAK_large_054.las',\n",
       " 'TEAK_large_055.las',\n",
       " 'TEAK_large_056.las',\n",
       " 'TEAK_large_057.las',\n",
       " 'TEAK_large_058.las',\n",
       " 'TEAK_large_059.las',\n",
       " 'TEAK_large_060.las',\n",
       " 'TEAK_large_061.las',\n",
       " 'TEAK_large_062.las',\n",
       " 'TEAK_large_063.las',\n",
       " 'TEAK_large_064.las',\n",
       " 'TEAK_large_065.las',\n",
       " 'TEAK_large_066.las',\n",
       " 'TEAK_large_067.las',\n",
       " 'TEAK_large_068.las',\n",
       " 'TEAK_large_069.las',\n",
       " 'TEAK_large_070.las',\n",
       " 'TEAK_large_071.las',\n",
       " 'TEAK_large_072.las',\n",
       " 'TEAK_large_073.las',\n",
       " 'TEAK_large_074.las',\n",
       " 'TEAK_large_075.las',\n",
       " 'TEAK_large_076.las',\n",
       " 'TEAK_large_077.las',\n",
       " 'TEAK_large_078.las',\n",
       " 'TEAK_large_079.las',\n",
       " 'TEAK_large_080.las',\n",
       " 'TEAK_large_081.las',\n",
       " 'TEAK_large_082.las',\n",
       " 'TEAK_large_083.las',\n",
       " 'TEAK_large_084.las',\n",
       " 'TEAK_large_085.las',\n",
       " 'TEAK_large_086.las',\n",
       " 'TEAK_large_087.las',\n",
       " 'TEAK_large_088.las',\n",
       " 'TEAK_large_089.las',\n",
       " 'TEAK_large_090.las',\n",
       " 'TEAK_large_091.las',\n",
       " 'TEAK_large_092.las',\n",
       " 'TEAK_large_093.las',\n",
       " 'TEAK_large_094.las',\n",
       " 'TEAK_large_095.las',\n",
       " 'TEAK_large_096.las',\n",
       " 'TEAK_large_097.las',\n",
       " 'TEAK_large_098.las',\n",
       " 'TEAK_large_099.las',\n",
       " 'TEAK_large_100.las',\n",
       " 'TEAK_large_101.las',\n",
       " 'TEAK_large_102.las',\n",
       " 'TEAK_large_103.las',\n",
       " 'TEAK_large_104.las',\n",
       " 'TEAK_large_105.las',\n",
       " 'TEAK_large_106.las',\n",
       " 'TEAK_large_107.las',\n",
       " 'TEAK_large_108.las',\n",
       " 'TEAK_large_109.las',\n",
       " 'TEAK_large_110.las',\n",
       " 'TEAK_large_111.las',\n",
       " 'TEAK_large_112.las',\n",
       " 'TEAK_large_113.las',\n",
       " 'TEAK_large_114.las',\n",
       " 'TEAK_large_115.las',\n",
       " 'TEAK_large_116.las',\n",
       " 'TEAK_large_117.las',\n",
       " 'TEAK_large_118.las',\n",
       " 'TEAK_large_119.las',\n",
       " 'TEAK_large_120.las',\n",
       " 'TEAK_large_121.las',\n",
       " 'TEAK_large_122.las',\n",
       " 'TEAK_large_123.las',\n",
       " 'TEAK_large_124.las',\n",
       " 'TEAK_large_125.las',\n",
       " 'TEAK_large_126.las',\n",
       " 'TEAK_large_127.las',\n",
       " 'TEAK_large_128.las',\n",
       " 'TEAK_large_129.las',\n",
       " 'TEAK_large_130.las',\n",
       " 'TEAK_large_131.las',\n",
       " 'TEAK_large_132.las',\n",
       " 'TEAK_large_133.las',\n",
       " 'TEAK_large_134.las',\n",
       " 'TEAK_large_135.las',\n",
       " 'TEAK_large_136.las',\n",
       " 'TEAK_large_137.las',\n",
       " 'TEAK_large_138.las',\n",
       " 'TEAK_large_139.las',\n",
       " 'TEAK_large_140.las',\n",
       " 'TEAK_large_141.las',\n",
       " 'TEAK_large_142.las',\n",
       " 'TEAK_large_143.las',\n",
       " 'TEAK_large_144.las',\n",
       " 'TEAK_large_145.las',\n",
       " 'TEAK_large_146.las',\n",
       " 'TEAK_large_147.las',\n",
       " 'TEAK_large_148.las',\n",
       " 'TEAK_large_149.las',\n",
       " 'TEAK_large_150.las',\n",
       " 'TEAK_large_151.las',\n",
       " 'TEAK_large_152.las',\n",
       " 'TEAK_large_153.las',\n",
       " 'TEAK_large_154.las',\n",
       " 'TEAK_large_155.las',\n",
       " 'TEAK_large_156.las',\n",
       " 'TEAK_large_157.las',\n",
       " 'TEAK_large_158.las',\n",
       " 'TEAK_large_159.las',\n",
       " 'TEAK_large_160.las',\n",
       " 'TEAK_large_161.las',\n",
       " 'TEAK_large_162.las',\n",
       " 'TEAK_large_163.las',\n",
       " 'TEAK_large_164.las',\n",
       " 'TEAK_large_165.las',\n",
       " 'TEAK_large_166.las',\n",
       " 'TEAK_large_167.las',\n",
       " 'TEAK_large_168.las',\n",
       " 'TEAK_large_169.las',\n",
       " 'TEAK_large_170.las',\n",
       " 'TEAK_large_171.las',\n",
       " 'TEAK_large_172.las',\n",
       " 'TEAK_large_173.las',\n",
       " 'TEAK_large_174.las',\n",
       " 'TEAK_large_175.las',\n",
       " 'TEAK_large_176.las',\n",
       " 'TEAK_large_177.las',\n",
       " 'TEAK_large_178.las',\n",
       " 'TEAK_large_179.las',\n",
       " 'TEAK_large_180.las',\n",
       " 'TEAK_large_181.las',\n",
       " 'TEAK_large_182.las',\n",
       " 'TEAK_large_183.las',\n",
       " 'TEAK_large_184.las',\n",
       " 'TEAK_large_185.las',\n",
       " 'TEAK_large_186.las',\n",
       " 'TEAK_large_187.las',\n",
       " 'TEAK_large_188.las',\n",
       " 'TEAK_large_189.las',\n",
       " 'TEAK_large_190.las',\n",
       " 'TEAK_large_191.las',\n",
       " 'TEAK_large_192.las',\n",
       " 'TEAK_large_193.las',\n",
       " 'TEAK_large_194.las',\n",
       " 'TEAK_large_195.las',\n",
       " 'TEAK_large_196.las',\n",
       " 'TEAK_large_197.las',\n",
       " 'TEAK_large_198.las',\n",
       " 'TEAK_large_199.las',\n",
       " 'TEAK_large_200.las',\n",
       " 'TEAK_large_201.las',\n",
       " 'TEAK_large_202.las',\n",
       " 'TEAK_large_203.las',\n",
       " 'TEAK_large_204.las',\n",
       " 'TEAK_large_205.las',\n",
       " 'TEAK_large_206.las',\n",
       " 'TEAK_large_207.las',\n",
       " 'TEAK_large_208.las',\n",
       " 'TEAK_large_209.las',\n",
       " 'TEAK_large_210.las',\n",
       " 'TEAK_large_211.las',\n",
       " 'TEAK_large_212.las',\n",
       " 'TEAK_large_213.las',\n",
       " 'TEAK_large_214.las',\n",
       " 'TEAK_large_215.las',\n",
       " 'TEAK_large_216.las',\n",
       " 'TEAK_large_217.las',\n",
       " 'TEAK_large_218.las',\n",
       " 'TEAK_large_219.las',\n",
       " 'TEAK_large_220.las',\n",
       " 'TEAK_large_221.las',\n",
       " 'TEAK_large_222.las',\n",
       " 'TEAK_large_223.las',\n",
       " 'TEAK_large_224.las',\n",
       " 'TEAK_large_225.las',\n",
       " 'TEAK_large_226.las',\n",
       " 'TEAK_large_227.las',\n",
       " 'TEAK_large_228.las',\n",
       " 'TEAK_large_229.las',\n",
       " 'TEAK_large_230.las',\n",
       " 'TEAK_large_231.las',\n",
       " 'TEAK_large_232.las',\n",
       " 'TEAK_large_233.las',\n",
       " 'TEAK_large_234.las',\n",
       " 'TEAK_large_235.las',\n",
       " 'TEAK_large_237.las',\n",
       " 'TEAK_large_238.las',\n",
       " 'TEAK_large_239.las',\n",
       " 'TEAK_large_240.las',\n",
       " 'TEAK_large_241.las',\n",
       " 'TEAK_large_242.las',\n",
       " 'TEAK_large_243.las',\n",
       " 'TEAK_large_244.las',\n",
       " 'TEAK_large_245.las',\n",
       " 'TEAK_large_246.las',\n",
       " 'TEAK_large_247.las',\n",
       " 'TEAK_large_248.las',\n",
       " 'TEAK_large_249.las']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1db50bc9-cbf3-4312-8029-44c635cfb89a",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_file = \"s3://maap-ops-workspace/jclayton0/normalized/norm_TEAK_047_lidar_2021.las\"\n",
    "url = build_file_url(files[145])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e4fc37c-fd68-4cd7-bbf3-101bf73ebca6",
   "metadata": {},
   "source": [
    "## Get JobIDs from previous submissions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7646e4c5-93e3-4884-8a92-f00a55f3c30d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "old_jobs_path = \"../my-private-bucket/run_output_20241115_131005/job_ids.txt\"\n",
    "old_output_dir = \"../my-private-bucket/run_output_20241115_131005/\"\n",
    "old_jobs = get_old_jobs_list(old_jobs_path)\n",
    "old_jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "7e88bf16-3cba-466d-b6ed-dceb2f031353",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['s3://maap-ops-workspace/jclayton0/sq_km_tiles_norm/TEAK_large_008.las',\n",
       " 's3://maap-ops-workspace/jclayton0/sq_km_tiles_norm/TEAK_large_009.las',\n",
       " 's3://maap-ops-workspace/jclayton0/sq_km_tiles_norm/TEAK_large_010.las',\n",
       " 's3://maap-ops-workspace/jclayton0/sq_km_tiles_norm/TEAK_large_011.las',\n",
       " 's3://maap-ops-workspace/jclayton0/sq_km_tiles_norm/TEAK_large_012.las',\n",
       " 's3://maap-ops-workspace/jclayton0/sq_km_tiles_norm/TEAK_large_013.las',\n",
       " 's3://maap-ops-workspace/jclayton0/sq_km_tiles_norm/TEAK_large_014.las',\n",
       " 's3://maap-ops-workspace/jclayton0/sq_km_tiles_norm/TEAK_large_055.las',\n",
       " 's3://maap-ops-workspace/jclayton0/sq_km_tiles_norm/TEAK_large_097.las']"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Get the failed job IDs, find the submission parameter locations,\n",
    "# and scrape the input file s3 url from the parameter JSON\n",
    "\n",
    "failed_job_ids = get_failed_jobs_in_list(old_jobs)\n",
    "failed_job_urls = [job_result_for(job) for job in failed_job_ids]\n",
    "failed_params = [to_failed_job_params(url) for url in failed_job_urls]\n",
    "failed_inputs = [get_failed_job_input_LAS(json) for json in failed_params]\n",
    "\n",
    "failed_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "0a693126-8178-4b04-9821-ef08f6652e8a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# resubmit_kwargs = [get_split_kwargs(url) for url in failed_inputs]\n",
    "# resubmit_kwargs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69901efc-1e69-488e-8d87-302ffe2e3d48",
   "metadata": {},
   "outputs": [],
   "source": [
    "failed_tiles = [tile.split('_norm/')[1] for tile in failed_inputs]\n",
    "failed_tiles\n",
    "#save them to a txtfile"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6b2c865-1a14-481f-b753-b3e24f809ecb",
   "metadata": {},
   "source": [
    "## Run Split on all files and collect the outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d94aac40-1587-416d-a420-d13aca238375",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting new model run at MAAP at 2024-11-15 13:10:05.350759.\n"
     ]
    }
   ],
   "source": [
    "start_time = datetime.datetime.now()\n",
    "\n",
    "# Set up output directory\n",
    "output_dir = Path(f\"/projects/my-private-bucket/run_output_\"\n",
    "                      f\"{start_time.strftime('%Y%m%d_%H%M%S')}\")\n",
    "os.makedirs(output_dir, exist_ok=False)\n",
    "\n",
    "# Set up log\n",
    "logging.basicConfig(filename=output_dir / \"run.log\",\n",
    "                        level=logging.INFO,\n",
    "                        format='%(asctime)s - %(message)s',\n",
    "                        datefmt='%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "log_and_print(f\"Starting new model run at MAAP at {start_time}.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "f28b6a9c-3eb7-4a6c-92af-2efe1d8f4acc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting new model run at MAAP at 2024-11-19 15:29:05.771577.\n",
      "Submitting 9 jobs.\n",
      "Submitted 9 jobs.\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for /: 'str' and 'str'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[78], line 39\u001b[0m\n\u001b[1;32m     36\u001b[0m job_ids \u001b[38;5;241m=\u001b[39m [job\u001b[38;5;241m.\u001b[39mid \u001b[38;5;28;01mfor\u001b[39;00m job \u001b[38;5;129;01min\u001b[39;00m jobs]\n\u001b[1;32m     38\u001b[0m \u001b[38;5;66;03m# Write job IDs to a file in case processing is interrupted\u001b[39;00m\n\u001b[0;32m---> 39\u001b[0m job_ids_file \u001b[38;5;241m=\u001b[39m \u001b[43moutput_dir\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mjob_ids.txt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(job_ids_file, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m     41\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m job_id \u001b[38;5;129;01min\u001b[39;00m job_ids:\n",
      "\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for /: 'str' and 'str'"
     ]
    }
   ],
   "source": [
    "    # Submit jobs for each pair of granules\n",
    "username = \"jclayton0\"\n",
    "job_limit = 1000\n",
    "check_interval = 90 #seconds between updates\n",
    "\n",
    "##### REMOVE this block if doing new run\n",
    "start_time = datetime.datetime.now()\n",
    "log_and_print(f\"Starting new model run at MAAP at {start_time}.\")\n",
    "output_dir = old_output_dir \n",
    "files = failed_inputs\n",
    "######\n",
    "\n",
    "\n",
    "    if job_limit:\n",
    "        n_jobs = min(len(files), job_limit)\n",
    "    else:\n",
    "        n_jobs = len(files)\n",
    "    log_and_print(f\"Submitting {n_jobs} \"\n",
    "                  f\"jobs.\")\n",
    "\n",
    "    job_kwargs_list = []\n",
    "    for file in files:\n",
    "        \n",
    "        #job_kwargs = get_split_kwargs(build_file_url(file))\n",
    "        job_kwargs = get_split_kwargs(build_file_url(file))\n",
    "\n",
    "        job_kwargs_list.append(job_kwargs)\n",
    "\n",
    "    jobs = []\n",
    "    for job_kwargs in job_kwargs_list[:job_limit]:\n",
    "        job = maap.submitJob(**job_kwargs)\n",
    "        jobs.append(job)\n",
    "\n",
    "    print(f\"Submitted {len(jobs)} jobs.\")\n",
    "\n",
    "    job_ids = [job.id for job in jobs]\n",
    "\n",
    "    # Write job IDs to a file in case processing is interrupted\n",
    "    job_ids_file = output_dir / \"job_ids.txt\"\n",
    "    with open(job_ids_file, 'w') as f:\n",
    "        for job_id in job_ids:\n",
    "            f.write(f\"{job_id}\\n\")\n",
    "    log_and_print(f\"Job IDs written to {job_ids_file}\")\n",
    "\n",
    "    # Give the jobs time to start\n",
    "    click.echo(\"Waiting for jobs to start...\")\n",
    "    time.sleep(10)\n",
    "\n",
    "    # Initialize job states\n",
    "    final_states = [\"Succeeded\", \"Failed\", \"Deleted\"]\n",
    "\n",
    "    job_states = {job_id: \"\" for job_id in job_ids}\n",
    "    update_job_states(job_states, final_states, batch_size=50, delay=10)\n",
    "\n",
    "    known_completed = len([state for state in job_states.values()\n",
    "                           if state in final_states])\n",
    "\n",
    "    while True:\n",
    "        try:\n",
    "            with tqdm(total=len(job_ids), desc=\"Jobs Completed\", unit=\"job\") as pbar:\n",
    "                while any(state not in final_states for state in job_states.values()):\n",
    "\n",
    "                    # Update the job states\n",
    "                    n_new_completed: int = update_job_states(job_states,\n",
    "                                                             final_states,\n",
    "                                                             batch_size = 50,\n",
    "                                                             delay = 10)\n",
    "\n",
    "                    # Update the progress bar\n",
    "                    pbar.update(n_new_completed)\n",
    "                    last_updated = datetime.datetime.now()\n",
    "                    known_completed += n_new_completed\n",
    "                    \n",
    "                    status_counts = {status: list(job_states.values()).count(status)\n",
    "                                     for status in final_states + [\"Accepted\", \"Running\"]}\n",
    "                    status_counts[\"Other\"] = len(job_states) - sum(status_counts.values())\n",
    "                    status_counts[\"Last updated\"] = last_updated.strftime(\"%H:%M:%S\")\n",
    "\n",
    "                    pbar.set_postfix(status_counts, refresh=True)\n",
    "\n",
    "                    if known_completed == len(job_ids):\n",
    "                        break\n",
    "\n",
    "                    time.sleep(check_interval)\n",
    "\n",
    "        except KeyboardInterrupt:\n",
    "            print(\"Are you sure you want to cancel the process?\")\n",
    "            print(\"Press Ctrl+C again to confirm, or wait to continue.\")\n",
    "            try:\n",
    "                time.sleep(3)\n",
    "                print(\"Continuing...\")\n",
    "            except KeyboardInterrupt:\n",
    "                print(\"Model run aborted.\")\n",
    "                pending_jobs = [job_id for job_id, state in job_states.items()\n",
    "                                if state not in final_states]\n",
    "                click.echo(f\"Cancelling {len(pending_jobs)} pending jobs.\")\n",
    "                for job_id in pending_jobs:\n",
    "                    maap.cancelJob(job_id)\n",
    "                break\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    # Process the results once all jobs are completed\n",
    "    succeeded_job_ids = get_succeeded_jobs_in_list(job_ids)\n",
    "    \n",
    "    failed_job_ids = get_failed_jobs_in_list(job_ids)\n",
    "\n",
    "    other_job_ids = get_other_status_jobs_in_list(job_ids) \n",
    "\n",
    "    click.echo(f\"Processing results for {len(succeeded_job_ids)} \"\n",
    "               f\"succeeded jobs.\")\n",
    "\n",
    "    click.echo(f\"Gathering tarball paths from succeeded jobs.\")\n",
    "\n",
    "    tar_paths = []\n",
    "    for job_id in tqdm(succeeded_job_ids):\n",
    "        job_result_url = job_result_for(job_id)\n",
    "        time.sleep(1) # to avoid overwhelming the API\n",
    "        job_output_dir = to_job_output_dir(job_result_url, username)\n",
    "        # Find .tar.gz file in the output dir\n",
    "        tar_file = [f for f in os.listdir(job_output_dir)\n",
    "                     if f.endswith('.tar.gz')]\n",
    "        if len(tar_file) > 1:\n",
    "            warnings.warn(f\"Multiple .tar.gz files found in \"\n",
    "                          f\"{job_output_dir}.\")\n",
    "        if len(tar_file) == 0:\n",
    "            warnings.warn(f\"No .tar.gz files found in \"\n",
    "                          f\"{job_output_dir}.\")\n",
    "        if tar_file:\n",
    "            tar_paths.append(os.path.join(job_output_dir, tar_file[0]))\n",
    "\n",
    "    # Log the succeeded and failed job IDs\n",
    "    logging.info(f\"{len(succeeded_job_ids)} jobs succeeded.\")\n",
    "    logging.info(f\"Succeeded job IDs: {succeeded_job_ids}\\n\")\n",
    "    logging.info(f\"{len(failed_job_ids)} jobs failed.\")\n",
    "    logging.info(f\"Failed job IDs: {failed_job_ids}\\n\")\n",
    "    logging.info(f\"{len(other_job_ids)} jobs in other states.\")\n",
    "    logging.info(f\"Other job IDs: {other_job_ids}\\n\")\n",
    "\n",
    "    # Copy all tarballs to the output directory\n",
    "    click.echo(f\"Copying {len(tar_paths)} Tarballs to {output_dir}.\")\n",
    "    copy_batch_count = 0\n",
    "    for tar_path in tqdm(tar_paths):\n",
    "        try:\n",
    "            shutil.copy(tar_path, output_dir)\n",
    "            copy_batch_count += 1\n",
    "            if copy_batch_count == 50:\n",
    "                time.sleep(60)\n",
    "                copy_batch_count = 0\n",
    "            else:\n",
    "                time.sleep(2)\n",
    "            \n",
    "        except Exception as e:\n",
    "            warnings.warn(f\"Error copying {tar_path} to {output_dir}: {str(e)}\")\n",
    "            click.echo(\"Retrying in 10 seconds.\")\n",
    "            time.sleep(10)\n",
    "            try:\n",
    "                shutil.copy(tar_path, output_dir)\n",
    "            except Exception as e:\n",
    "                click.echo(f\"Retry failed: {str(e)}\")\n",
    "                click.echo(f\"Skipping {tar_path}.\")\n",
    "                continue\n",
    "\n",
    "    # Compress the output directory\n",
    "    # click.echo(f\"Compressing output directory.\")\n",
    "    # shutil.make_archive(output_dir, 'zip', output_dir)\n",
    "    # click.echo(f\"Output directory compressed to {output_dir}.zip.\")\n",
    "\n",
    "    end_time = datetime.datetime.now()\n",
    "\n",
    "    log_and_print(f\"Model run completed at {end_time}.\")\n",
    "    \n",
    "# if __name__ == \"__main__\":\n",
    "#     main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "d0725fd7-5b94-4b42-b951-534476dc452f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a39f19f7-b9dc-4f81-aaf9-d2a7d7d4bf0b',\n",
       " '43bb3357-a447-41a6-8d18-2c24947e349e',\n",
       " '886c04df-acbc-4e3e-8807-356951c86da9',\n",
       " '0e238056-d343-4924-b8ff-7e04d0fb7224',\n",
       " '7af49420-7269-4955-b6f9-2565849d8be1',\n",
       " '96001cf0-2a46-469f-9f40-d7335e8823d5',\n",
       " 'cf180c75-0169-4c11-a8dc-96ae5bb1d4be',\n",
       " '7737a35c-5724-40b4-a352-b479b14df366',\n",
       " '668f99ba-5fca-4980-a82b-276d06681ca5']"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_job_ids = job_ids\n",
    "new_job_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "431f5407-b337-42ce-8c63-028c6c517584",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing results for 239 succeeded jobs.\n",
      "Gathering tarball paths from succeeded jobs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 239/239 [04:49<00:00,  1.21s/it]\n"
     ]
    }
   ],
   "source": [
    "# comment out if doing new stuff\n",
    "job_ids = old_jobs\n",
    "output_dir = old_output_dir\n",
    "username = \"jclayton0\"\n",
    "\n",
    "# Process the results once all jobs are completed\n",
    "succeeded_job_ids = [job_id for job_id in job_ids\n",
    "                     if job_status_for(job_id) == \"Succeeded\"]\n",
    "\n",
    "failed_job_ids = [job_id for job_id in job_ids\n",
    "                  if job_status_for(job_id) == \"Failed\"]\n",
    "\n",
    "other_job_ids = [job_id for job_id in job_ids\n",
    "                 if job_status_for(job_id)\n",
    "                 not in [\"Succeeded\", \"Failed\"]]\n",
    "\n",
    "click.echo(f\"Processing results for {len(succeeded_job_ids)} \"\n",
    "           f\"succeeded jobs.\")\n",
    "\n",
    "click.echo(f\"Gathering tarball paths from succeeded jobs.\")\n",
    "\n",
    "tar_paths = []\n",
    "for job_id in tqdm(succeeded_job_ids):\n",
    "    job_result_url = job_result_for(job_id)\n",
    "    time.sleep(1) # to avoid overwhelming the API\n",
    "    job_output_dir = to_job_output_dir(job_result_url, username)\n",
    "    # Find .tar.gz file in the output dir\n",
    "    tar_file = [f for f in os.listdir(job_output_dir)\n",
    "                 if f.endswith('.tar.gz')]\n",
    "    if len(tar_file) > 1:\n",
    "        warnings.warn(f\"Multiple .tar.gz files found in \"\n",
    "                      f\"{job_output_dir}.\")\n",
    "    if len(tar_file) == 0:\n",
    "        warnings.warn(f\"No .tar.gz files found in \"\n",
    "                      f\"{job_output_dir}.\")\n",
    "    if tar_file:\n",
    "        tar_paths.append(os.path.join(job_output_dir, tar_file[0]))\n",
    "\n",
    "# Log the succeeded and failed job IDs\n",
    "logging.info(f\"{len(succeeded_job_ids)} jobs succeeded.\")\n",
    "logging.info(f\"Succeeded job IDs: {succeeded_job_ids}\\n\")\n",
    "logging.info(f\"{len(failed_job_ids)} jobs failed.\")\n",
    "logging.info(f\"Failed job IDs: {failed_job_ids}\\n\")\n",
    "logging.info(f\"{len(other_job_ids)} jobs in other states.\")\n",
    "logging.info(f\"Other job IDs: {other_job_ids}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6e9d6938-1d1a-4d81-968c-41bdd611ca0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## pickle tarpaths \n",
    "\n",
    "import pickle\n",
    "\n",
    "# with open('tar_paths_step_1.pkl', 'wb') as f:\n",
    "#    pickle.dump(tar_paths, f)\n",
    "with open('tar_paths_step_1.pkl', 'rb') as f:\n",
    "    tar_paths = pickle.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6d323c19-3e70-468a-adc3-196d7a65bcbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying 239 Tarballs to ../my-private-bucket/run_output_20241115_131005/.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 239/239 [07:29<00:00,  1.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model run completed at 2024-11-19 20:15:00.425728.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "output_dir = old_output_dir\n",
    "outdir_files = os.listdir(output_dir)\n",
    "# Copy all tarballs to the output directory\n",
    "click.echo(f\"Copying {len(tar_paths)} Tarballs to {output_dir}.\")\n",
    "copy_batch_count = 0\n",
    "for tar_path in tqdm(tar_paths):\n",
    "    fname = tar_path.split('/')[-1]\n",
    "    if fname in outdir_files:\n",
    "        copy_batch_count +=1\n",
    "        continue\n",
    "    else:\n",
    "        try:\n",
    "            shutil.copy(tar_path, output_dir)\n",
    "            copy_batch_count += 1\n",
    "            if copy_batch_count == 10:\n",
    "                time.sleep(60)\n",
    "                copy_batch_count = 0\n",
    "            else:\n",
    "                time.sleep(2)\n",
    "            \n",
    "        except Exception as e:\n",
    "            warnings.warn(f\"Error copying {tar_path} to {output_dir}: {str(e)}\")\n",
    "            click.echo(\"Retrying in 10 seconds.\")\n",
    "            time.sleep(10)\n",
    "            try:\n",
    "                shutil.copy(tar_path, output_dir)\n",
    "            except Exception as e:\n",
    "                click.echo(f\"Retry failed: {str(e)}\")\n",
    "                click.echo(f\"Skipping {tar_path}.\")\n",
    "                continue\n",
    "\n",
    "\n",
    "# Compress the output directory\n",
    "# click.echo(f\"Compressing output directory.\")\n",
    "# shutil.make_archive(output_dir, 'zip', output_dir)\n",
    "# click.echo(f\"Output directory compressed to {output_dir}.zip.\")\n",
    "\n",
    "end_time = datetime.datetime.now()\n",
    "\n",
    "log_and_print(f\"Model run completed at {end_time}.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1230ff45-f2e6-489d-b91e-b0561c4ee16a",
   "metadata": {},
   "source": [
    "## Above, split all files in the directory by submitting a split job then gather the results into a single directory\n",
    "\n",
    "## This has to be run several times. You also need, in the terminal, to type ls -lhS | tac and remove any files that have a size of 0; they need to be re-transferred\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e27fd45-e37e-4f1e-b9d5-67d17f0b0825",
   "metadata": {},
   "source": [
    "## Then, run extract_tarballs.sh after modifying it to include the correct directory path. \n",
    "\n",
    "## It will take some TIME"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec2343a8-210f-4aaf-9b09-e456faa5f4c6",
   "metadata": {},
   "source": [
    "## Now we below test batching 1460 jobs for one tile. This will need to be re-written as a for loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "da94b053-852b-444a-adf6-18b32d03c8eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../my-private-bucket/run_output_20241115_131005/TEAK_large_016/'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Directory containing your files\n",
    "output_dir = old_output_dir\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a6d5082-5813-448c-acbe-b1e0d5a872f8",
   "metadata": {},
   "source": [
    "## What is gone:\n",
    "\n",
    "* Code to make the kwargs from the directory\n",
    "* Code to save the kwargs as pickles\n",
    "* Code to read the kwargs from pickles and submit the jobs\n",
    "* Any other code and notes I had there."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1538bea-e685-433e-a45a-23809df76c39",
   "metadata": {},
   "source": [
    "## What to do: \n",
    "\n",
    "1. Restart the shell script\n",
    "2. Make internal hyperlinks\n",
    "3. Plan every part of the rest of this\n",
    "4. rewrite missing code\n",
    "5. Push to github regularly\n",
    "6. Something fucking else that I don't remember\n",
    "7. List it all, 100% of everything that needs to happen for sure. What else though. There was something fucking else. it will come back to me.\n",
    "8. Change step 1 so that it does not tarball things."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9dccd8d-ce39-4202-9ef7-68e5713da2a4",
   "metadata": {},
   "source": [
    "# What does the script still need to do?\n",
    "\n",
    "1. Missing old parts up to submitting the segmentation jobs\n",
    "2. Check every job ID for missing, resubmit the failed, and then overwrite the failed IDs\n",
    "   * probably this looks like copying the successful IDs over and adding the newly submitted IDs to that list, then saving that list in the same file\n",
    "3. Move all the segmented tiles into one directory and tarball them\n",
    "   * this looks like what I did before: gather the paths for each and save them to a pkl, then move them en masse to new dir.\n",
    "   * Then this looks like: tarballing them all\n",
    "4. Then write a script to gather all the tarball paths and make kwargs from them\n",
    "5. Then submit these all as jobs to step 3\n",
    "6. Then gather outputs and move them, again.\n",
    "7. Finally submit everything that's left to step 4.\n",
    "8. Gather Step 4 outputs and then make the CSV. Probably on our server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cdfcf51-a828-47da-8d9b-a28727e222c9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.2.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
