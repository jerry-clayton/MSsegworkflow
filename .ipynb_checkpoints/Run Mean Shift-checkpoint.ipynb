{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ec571ca1-3510-43d8-a825-d06b203d47e3",
   "metadata": {},
   "source": [
    "# Mean Shift Segmentation Batch Runner\n",
    "\n",
    "Author: [Jerry Clayton](https://github.com/jerry-clayton)\n",
    "\n",
    "For: [Ni-Meister Lab](http://www.geography.hunter.cuny.edu/~wenge/)\n",
    "\n",
    "Adapted from [Ian Grant's Script](https://github.com/i-c-grant/ni-meister-gedi-biomass/blob/main/run_on_maap.py)\n",
    "\n",
    "#### This Jupyter notebook handles the batch processing of the modified Mean Shift tree segmentation workflow developed by the author and Dr. Ni-Meister on the NASA MAAP platform. \n",
    "\n",
    "#### The [AMS3D](https://www.sciencedirect.com/science/article/abs/pii/S0034425716302292) was first proposed by Ferraz et. al, and this implementation depends on Dr. Nikolai Knapp's [MeanShiftR](https://github.com/niknap/MeanShiftR/tree/master/R) package\n",
    "\n",
    "#### This workflow is executed in four parts: Split, Segment, Reconcile, and Merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "86c89e97-56ac-49e7-a753-89bb0f6b588a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import logging\n",
    "import os\n",
    "import shutil\n",
    "import time\n",
    "import glob\n",
    "import tarfile\n",
    "\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "from typing import Dict, List\n",
    "\n",
    "import click\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from geopandas import GeoDataFrame\n",
    "from maap.maap import MAAP\n",
    "from maap.Result import Granule\n",
    "\n",
    "maap = MAAP(maap_host='api.maap-project.org')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ec26e015-e5b3-492d-92d5-985834636a3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_file_url(filename):\n",
    "    url_first_part = \"s3://maap-ops-workspace/jclayton0/sq_km_tiles_norm/\" \n",
    "    url = f'{url_first_part}{filename}'\n",
    "    return url\n",
    "\n",
    "def build_test_file_url(filename):\n",
    "    url_first_part = \"s3://maap-ops-workspace/jclayton0/test-input-sm-tiles/\" \n",
    "    url = f'{url_first_part}{filename}'\n",
    "    return url\n",
    "\n",
    "def get_split_kwargs(fileurl):\n",
    "     job_kwargs = {\n",
    "            \"identifier\": \"Mean Shift Split LAS\",\n",
    "            \"algo_id\": \"MS-Step-1-Split\",\n",
    "            \"version\": \"main\",\n",
    "            \"username\": \"jclayton0\",\n",
    "            \"queue\": \"maap-dps-worker-64gb\",\n",
    "            \"LAS\": fileurl,\n",
    "            \"Subplot_width\": 25,\n",
    "            \"Buffer_width\": 10\n",
    "    }\n",
    "    \n",
    "     return job_kwargs\n",
    "\n",
    "def get_segment_kwargs(fileurl):\n",
    "    job_kwargs = {\n",
    "            \"identifier\": \"Mean Shift Segment LAS\",\n",
    "            \"algo_id\": \"MS-Step-2-Segment-v2\",\n",
    "            \"version\": \"main\",\n",
    "            \"username\": \"jclayton0\",\n",
    "            \"queue\": \"maap-dps-worker-64gb\",\n",
    "            \"Point Cloud RDS\": fileurl,\n",
    "            \"Subplot_widthFrac_cores\": 0.9\n",
    "    }\n",
    "    \n",
    "    return job_kwargs\n",
    "\n",
    "def get_reconcile_kwargs(tarball_url, las_url):\n",
    "    job_kwargs = {\n",
    "            \"identifier\": \"Mean Shift Reconcile LAS\",\n",
    "            \"algo_id\": \"MS-Step-3-Reconcile-v3\",\n",
    "            \"version\": \"main\",\n",
    "            \"username\": \"jclayton0\",\n",
    "            \"queue\": \"maap-dps-worker-64gb\",\n",
    "            \"tarball\": tarball_url,\n",
    "            \"original_las\": las_url\n",
    "    }\n",
    "    \n",
    "    return job_kwargs\n",
    "\n",
    "def get_merge_kwargs(fileurl):\n",
    "    job_kwargs = {\n",
    "            \"identifier\": \"Mean Shift Merge Trees\",\n",
    "            \"algo_id\": \"MS-Step-4-Merge\",\n",
    "            \"version\": \"main\",\n",
    "            \"username\": \"jclayton0\",\n",
    "            \"queue\": \"maap-dps-worker-64gb\",\n",
    "            \"segmented_las\": fileurl\n",
    "    }\n",
    "    \n",
    "    return job_kwargs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bed982cd-9128-4a5c-bbef-8217ab5c79ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def local_url_to_s3(url):\n",
    "    second = str.split(url,\"my-private-bucket\")[1]\n",
    "    full = f\"s3://maap-ops-workspace/jclayton0{second}\"\n",
    "    return full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3b8cc621-e4b1-4eb2-835b-1e87229ddaf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def job_status_for(job_id: str) -> str:\n",
    "    return maap.getJobStatus(job_id)\n",
    "\n",
    "def job_result_for(job_id: str) -> str:\n",
    "    return maap.getJobResult(job_id)[0]\n",
    "\n",
    "def to_job_output_dir(job_result_url: str, username: str) -> str:\n",
    "    return (f\"/projects/my-private-bucket/\"\n",
    "            f\"{job_result_url.split(f'/{username}/')[1]}\")\n",
    "\n",
    "def log_and_print(message: str):\n",
    "    logging.info(message)\n",
    "    click.echo(message)\n",
    "\n",
    "def update_job_states(job_states: Dict[str, str],\n",
    "                      final_states: List[str],\n",
    "                      batch_size: int,\n",
    "                      delay: int) -> Dict[str, str]:\n",
    "    \"\"\"Update the job states dictionary in place.\n",
    "\n",
    "    Updating occurs in batches, with a delay in seconds between batches.\n",
    "\n",
    "    Return the number of jobs updated to final states.\n",
    "    \"\"\"\n",
    "    batch_count = 0\n",
    "    n_updated_to_final = 0\n",
    "    for job_id, state in job_states.items():\n",
    "        if state not in final_states:\n",
    "            new_state: str = job_status_for(job_id)\n",
    "            job_states[job_id] = new_state\n",
    "            if new_state in final_states:\n",
    "                n_updated_to_final += 1\n",
    "            batch_count += 1\n",
    "        # Sleep after each batch to avoid overwhelming the API\n",
    "        if batch_count == batch_size:\n",
    "            time.sleep(delay)\n",
    "            batch_count = 0\n",
    "\n",
    "    return n_updated_to_final\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c52d6c25-afac-41c9-8d88-1420ef96bd40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'code': 200,\n",
       " 'message': 'success',\n",
       " 'queues': ['maap-dps-sandbox',\n",
       "  'maap-dps-worker-64gb',\n",
       "  'maap-dps-cuny-worker-512gb',\n",
       "  'maap-dps-worker-32vcpu-64gb',\n",
       "  'maap-dps-worker-32gb',\n",
       "  'maap-dps-worker-8gb',\n",
       "  'maap-dps-worker-16gb']}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "maap.getQueues().json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "61b6f73c-e145-4280-80e5-0dab269107ff",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Source Dir for large tiles, relative path\n",
    "long_list = glob.glob('../my-private-bucket/sq_km_tiles_norm/*')\n",
    "test_list = glob.glob('../my-private-bucket/test-input-sm-tiles/*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7adb7b7e-0dfd-4480-8126-2b999e4a2fc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "files = list()\n",
    "for file in long_list:\n",
    "    files.append(str.split(file, '/')[3])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "23c38d1f-fab1-4e79-a02c-00a54c645222",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['TEAK_large_001.las',\n",
       " 'TEAK_large_002.las',\n",
       " 'TEAK_large_003.las',\n",
       " 'TEAK_large_004.las',\n",
       " 'TEAK_large_005.las',\n",
       " 'TEAK_large_006.las',\n",
       " 'TEAK_large_007.las',\n",
       " 'TEAK_large_008.las',\n",
       " 'TEAK_large_009.las',\n",
       " 'TEAK_large_010.las',\n",
       " 'TEAK_large_011.las',\n",
       " 'TEAK_large_012.las',\n",
       " 'TEAK_large_013.las',\n",
       " 'TEAK_large_014.las',\n",
       " 'TEAK_large_015.las',\n",
       " 'TEAK_large_016.las',\n",
       " 'TEAK_large_017.las',\n",
       " 'TEAK_large_018.las',\n",
       " 'TEAK_large_019.las',\n",
       " 'TEAK_large_020.las',\n",
       " 'TEAK_large_021.las',\n",
       " 'TEAK_large_022.las',\n",
       " 'TEAK_large_023.las',\n",
       " 'TEAK_large_024.las',\n",
       " 'TEAK_large_025.las',\n",
       " 'TEAK_large_026.las',\n",
       " 'TEAK_large_027.las',\n",
       " 'TEAK_large_028.las',\n",
       " 'TEAK_large_029.las',\n",
       " 'TEAK_large_030.las',\n",
       " 'TEAK_large_031.las',\n",
       " 'TEAK_large_032.las',\n",
       " 'TEAK_large_033.las',\n",
       " 'TEAK_large_034.las',\n",
       " 'TEAK_large_035.las',\n",
       " 'TEAK_large_036.las',\n",
       " 'TEAK_large_037.las',\n",
       " 'TEAK_large_038.las',\n",
       " 'TEAK_large_039.las',\n",
       " 'TEAK_large_040.las',\n",
       " 'TEAK_large_041.las',\n",
       " 'TEAK_large_042.las',\n",
       " 'TEAK_large_043.las',\n",
       " 'TEAK_large_044.las',\n",
       " 'TEAK_large_045.las',\n",
       " 'TEAK_large_046.las',\n",
       " 'TEAK_large_047.las',\n",
       " 'TEAK_large_048.las',\n",
       " 'TEAK_large_049.las',\n",
       " 'TEAK_large_050.las',\n",
       " 'TEAK_large_051.las',\n",
       " 'TEAK_large_052.las',\n",
       " 'TEAK_large_053.las',\n",
       " 'TEAK_large_054.las',\n",
       " 'TEAK_large_055.las',\n",
       " 'TEAK_large_056.las',\n",
       " 'TEAK_large_057.las',\n",
       " 'TEAK_large_058.las',\n",
       " 'TEAK_large_059.las',\n",
       " 'TEAK_large_060.las',\n",
       " 'TEAK_large_061.las',\n",
       " 'TEAK_large_062.las',\n",
       " 'TEAK_large_063.las',\n",
       " 'TEAK_large_064.las',\n",
       " 'TEAK_large_065.las',\n",
       " 'TEAK_large_066.las',\n",
       " 'TEAK_large_067.las',\n",
       " 'TEAK_large_068.las',\n",
       " 'TEAK_large_069.las',\n",
       " 'TEAK_large_070.las',\n",
       " 'TEAK_large_071.las',\n",
       " 'TEAK_large_072.las',\n",
       " 'TEAK_large_073.las',\n",
       " 'TEAK_large_074.las',\n",
       " 'TEAK_large_075.las',\n",
       " 'TEAK_large_076.las',\n",
       " 'TEAK_large_077.las',\n",
       " 'TEAK_large_078.las',\n",
       " 'TEAK_large_079.las',\n",
       " 'TEAK_large_080.las',\n",
       " 'TEAK_large_081.las',\n",
       " 'TEAK_large_082.las',\n",
       " 'TEAK_large_083.las',\n",
       " 'TEAK_large_084.las',\n",
       " 'TEAK_large_085.las',\n",
       " 'TEAK_large_086.las',\n",
       " 'TEAK_large_087.las',\n",
       " 'TEAK_large_088.las',\n",
       " 'TEAK_large_089.las',\n",
       " 'TEAK_large_090.las',\n",
       " 'TEAK_large_091.las',\n",
       " 'TEAK_large_092.las',\n",
       " 'TEAK_large_093.las',\n",
       " 'TEAK_large_094.las',\n",
       " 'TEAK_large_095.las',\n",
       " 'TEAK_large_096.las',\n",
       " 'TEAK_large_097.las',\n",
       " 'TEAK_large_098.las',\n",
       " 'TEAK_large_099.las',\n",
       " 'TEAK_large_100.las',\n",
       " 'TEAK_large_101.las',\n",
       " 'TEAK_large_102.las',\n",
       " 'TEAK_large_103.las',\n",
       " 'TEAK_large_104.las',\n",
       " 'TEAK_large_105.las',\n",
       " 'TEAK_large_106.las',\n",
       " 'TEAK_large_107.las',\n",
       " 'TEAK_large_108.las',\n",
       " 'TEAK_large_109.las',\n",
       " 'TEAK_large_110.las',\n",
       " 'TEAK_large_111.las',\n",
       " 'TEAK_large_112.las',\n",
       " 'TEAK_large_113.las',\n",
       " 'TEAK_large_114.las',\n",
       " 'TEAK_large_115.las',\n",
       " 'TEAK_large_116.las',\n",
       " 'TEAK_large_117.las',\n",
       " 'TEAK_large_118.las',\n",
       " 'TEAK_large_119.las',\n",
       " 'TEAK_large_120.las',\n",
       " 'TEAK_large_121.las',\n",
       " 'TEAK_large_122.las',\n",
       " 'TEAK_large_123.las',\n",
       " 'TEAK_large_124.las',\n",
       " 'TEAK_large_125.las',\n",
       " 'TEAK_large_126.las',\n",
       " 'TEAK_large_127.las',\n",
       " 'TEAK_large_128.las',\n",
       " 'TEAK_large_129.las',\n",
       " 'TEAK_large_130.las',\n",
       " 'TEAK_large_131.las',\n",
       " 'TEAK_large_132.las',\n",
       " 'TEAK_large_133.las',\n",
       " 'TEAK_large_134.las',\n",
       " 'TEAK_large_135.las',\n",
       " 'TEAK_large_136.las',\n",
       " 'TEAK_large_137.las',\n",
       " 'TEAK_large_138.las',\n",
       " 'TEAK_large_139.las',\n",
       " 'TEAK_large_140.las',\n",
       " 'TEAK_large_141.las',\n",
       " 'TEAK_large_142.las',\n",
       " 'TEAK_large_143.las',\n",
       " 'TEAK_large_144.las',\n",
       " 'TEAK_large_145.las',\n",
       " 'TEAK_large_146.las',\n",
       " 'TEAK_large_147.las',\n",
       " 'TEAK_large_148.las',\n",
       " 'TEAK_large_149.las',\n",
       " 'TEAK_large_150.las',\n",
       " 'TEAK_large_151.las',\n",
       " 'TEAK_large_152.las',\n",
       " 'TEAK_large_153.las',\n",
       " 'TEAK_large_154.las',\n",
       " 'TEAK_large_155.las',\n",
       " 'TEAK_large_156.las',\n",
       " 'TEAK_large_157.las',\n",
       " 'TEAK_large_158.las',\n",
       " 'TEAK_large_159.las',\n",
       " 'TEAK_large_160.las',\n",
       " 'TEAK_large_161.las',\n",
       " 'TEAK_large_162.las',\n",
       " 'TEAK_large_163.las',\n",
       " 'TEAK_large_164.las',\n",
       " 'TEAK_large_165.las',\n",
       " 'TEAK_large_166.las',\n",
       " 'TEAK_large_167.las',\n",
       " 'TEAK_large_168.las',\n",
       " 'TEAK_large_169.las',\n",
       " 'TEAK_large_170.las',\n",
       " 'TEAK_large_171.las',\n",
       " 'TEAK_large_172.las',\n",
       " 'TEAK_large_173.las',\n",
       " 'TEAK_large_174.las',\n",
       " 'TEAK_large_175.las',\n",
       " 'TEAK_large_176.las',\n",
       " 'TEAK_large_177.las',\n",
       " 'TEAK_large_178.las',\n",
       " 'TEAK_large_179.las',\n",
       " 'TEAK_large_180.las',\n",
       " 'TEAK_large_181.las',\n",
       " 'TEAK_large_182.las',\n",
       " 'TEAK_large_183.las',\n",
       " 'TEAK_large_184.las',\n",
       " 'TEAK_large_185.las',\n",
       " 'TEAK_large_186.las',\n",
       " 'TEAK_large_187.las',\n",
       " 'TEAK_large_188.las',\n",
       " 'TEAK_large_189.las',\n",
       " 'TEAK_large_190.las',\n",
       " 'TEAK_large_191.las',\n",
       " 'TEAK_large_192.las',\n",
       " 'TEAK_large_193.las',\n",
       " 'TEAK_large_194.las',\n",
       " 'TEAK_large_195.las',\n",
       " 'TEAK_large_196.las',\n",
       " 'TEAK_large_197.las',\n",
       " 'TEAK_large_198.las',\n",
       " 'TEAK_large_199.las',\n",
       " 'TEAK_large_200.las',\n",
       " 'TEAK_large_201.las',\n",
       " 'TEAK_large_202.las',\n",
       " 'TEAK_large_203.las',\n",
       " 'TEAK_large_204.las',\n",
       " 'TEAK_large_205.las',\n",
       " 'TEAK_large_206.las',\n",
       " 'TEAK_large_207.las',\n",
       " 'TEAK_large_208.las',\n",
       " 'TEAK_large_209.las',\n",
       " 'TEAK_large_210.las',\n",
       " 'TEAK_large_211.las',\n",
       " 'TEAK_large_212.las',\n",
       " 'TEAK_large_213.las',\n",
       " 'TEAK_large_214.las',\n",
       " 'TEAK_large_215.las',\n",
       " 'TEAK_large_216.las',\n",
       " 'TEAK_large_217.las',\n",
       " 'TEAK_large_218.las',\n",
       " 'TEAK_large_219.las',\n",
       " 'TEAK_large_220.las',\n",
       " 'TEAK_large_221.las',\n",
       " 'TEAK_large_222.las',\n",
       " 'TEAK_large_223.las',\n",
       " 'TEAK_large_224.las',\n",
       " 'TEAK_large_225.las',\n",
       " 'TEAK_large_226.las',\n",
       " 'TEAK_large_227.las',\n",
       " 'TEAK_large_228.las',\n",
       " 'TEAK_large_229.las',\n",
       " 'TEAK_large_230.las',\n",
       " 'TEAK_large_231.las',\n",
       " 'TEAK_large_232.las',\n",
       " 'TEAK_large_233.las',\n",
       " 'TEAK_large_234.las',\n",
       " 'TEAK_large_235.las',\n",
       " 'TEAK_large_237.las',\n",
       " 'TEAK_large_238.las',\n",
       " 'TEAK_large_239.las',\n",
       " 'TEAK_large_240.las',\n",
       " 'TEAK_large_241.las',\n",
       " 'TEAK_large_242.las',\n",
       " 'TEAK_large_243.las',\n",
       " 'TEAK_large_244.las',\n",
       " 'TEAK_large_245.las',\n",
       " 'TEAK_large_246.las',\n",
       " 'TEAK_large_247.las',\n",
       " 'TEAK_large_248.las',\n",
       " 'TEAK_large_249.las']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e26bae77-8856-414c-93ab-10c775197ef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_file = \"s3://maap-ops-workspace/jclayton0/normalized/norm_TEAK_047_lidar_2021.las\"\n",
    "url = build_file_url(files[145])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25d572a4-2845-4028-9285-4344d49b8983",
   "metadata": {},
   "source": [
    "## Run Split on all files and collect the outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e309b72d-dcdc-4e0d-85c9-9de9d3257063",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting new model run at MAAP at 2024-11-15 12:50:06.899425.\n"
     ]
    }
   ],
   "source": [
    "start_time = datetime.datetime.now()\n",
    "\n",
    "# Set up output directory\n",
    "output_dir = Path(f\"/projects/my-private-bucket/run_output_\"\n",
    "                      f\"{start_time.strftime('%Y%m%d_%H%M%S')}\")\n",
    "os.makedirs(output_dir, exist_ok=False)\n",
    "\n",
    "# Set up log\n",
    "logging.basicConfig(filename=output_dir / \"run.log\",\n",
    "                        level=logging.INFO,\n",
    "                        format='%(asctime)s - %(message)s',\n",
    "                        datefmt='%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "log_and_print(f\"Starting new model run at MAAP at {start_time}.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb7b2b02-b1bd-46a9-8920-68c08023d586",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submitting 20 jobs.\n",
      "Submitted 20 jobs.\n",
      "Job IDs written to /projects/my-private-bucket/run_output_20241115_125006/job_ids.txt\n",
      "Waiting for jobs to start...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Jobs Completed:  35%|███▌      | 7/20 [07:36<02:49, 13.03s/job, Succeeded=7, Failed=0, Deleted=0, Accepted=0, Running=13, Other=0, Last updated=12:57:57]"
     ]
    }
   ],
   "source": [
    "    # Submit jobs for each pair of granules\n",
    "username = \"jclayton0\"\n",
    "job_limit = 20\n",
    "check_interval = 90 #seconds between updates\n",
    "    if job_limit:\n",
    "        n_jobs = min(len(files), job_limit)\n",
    "    else:\n",
    "        n_jobs = len(files)\n",
    "    log_and_print(f\"Submitting {n_jobs} \"\n",
    "                  f\"jobs.\")\n",
    "\n",
    "    job_kwargs_list = []\n",
    "    for file in files:\n",
    "        \n",
    "        #job_kwargs = get_split_kwargs(build_file_url(file))\n",
    "        job_kwargs = get_split_kwargs(build_file_url(file))\n",
    "\n",
    "        job_kwargs_list.append(job_kwargs)\n",
    "\n",
    "    jobs = []\n",
    "    for job_kwargs in job_kwargs_list[:job_limit]:\n",
    "        job = maap.submitJob(**job_kwargs)\n",
    "        jobs.append(job)\n",
    "\n",
    "    print(f\"Submitted {len(jobs)} jobs.\")\n",
    "\n",
    "    job_ids = [job.id for job in jobs]\n",
    "\n",
    "    # Write job IDs to a file in case processing is interrupted\n",
    "    job_ids_file = output_dir / \"job_ids.txt\"\n",
    "    with open(job_ids_file, 'w') as f:\n",
    "        for job_id in job_ids:\n",
    "            f.write(f\"{job_id}\\n\")\n",
    "    log_and_print(f\"Job IDs written to {job_ids_file}\")\n",
    "\n",
    "    # Give the jobs time to start\n",
    "    click.echo(\"Waiting for jobs to start...\")\n",
    "    time.sleep(10)\n",
    "\n",
    "    # Initialize job states\n",
    "    final_states = [\"Succeeded\", \"Failed\", \"Deleted\"]\n",
    "\n",
    "    job_states = {job_id: \"\" for job_id in job_ids}\n",
    "    update_job_states(job_states, final_states, batch_size=50, delay=10)\n",
    "\n",
    "    known_completed = len([state for state in job_states.values()\n",
    "                           if state in final_states])\n",
    "\n",
    "    while True:\n",
    "        try:\n",
    "            with tqdm(total=len(job_ids), desc=\"Jobs Completed\", unit=\"job\") as pbar:\n",
    "                while any(state not in final_states for state in job_states.values()):\n",
    "\n",
    "                    # Update the job states\n",
    "                    n_new_completed: int = update_job_states(job_states,\n",
    "                                                             final_states,\n",
    "                                                             batch_size = 50,\n",
    "                                                             delay = 10)\n",
    "\n",
    "                    # Update the progress bar\n",
    "                    pbar.update(n_new_completed)\n",
    "                    last_updated = datetime.datetime.now()\n",
    "                    known_completed += n_new_completed\n",
    "                    \n",
    "                    status_counts = {status: list(job_states.values()).count(status)\n",
    "                                     for status in final_states + [\"Accepted\", \"Running\"]}\n",
    "                    status_counts[\"Other\"] = len(job_states) - sum(status_counts.values())\n",
    "                    status_counts[\"Last updated\"] = last_updated.strftime(\"%H:%M:%S\")\n",
    "\n",
    "                    pbar.set_postfix(status_counts, refresh=True)\n",
    "\n",
    "                    if known_completed == len(job_ids):\n",
    "                        break\n",
    "\n",
    "                    time.sleep(check_interval)\n",
    "\n",
    "        except KeyboardInterrupt:\n",
    "            print(\"Are you sure you want to cancel the process?\")\n",
    "            print(\"Press Ctrl+C again to confirm, or wait to continue.\")\n",
    "            try:\n",
    "                time.sleep(3)\n",
    "                print(\"Continuing...\")\n",
    "            except KeyboardInterrupt:\n",
    "                print(\"Model run aborted.\")\n",
    "                pending_jobs = [job_id for job_id, state in job_states.items()\n",
    "                                if state not in final_states]\n",
    "                click.echo(f\"Cancelling {len(pending_jobs)} pending jobs.\")\n",
    "                for job_id in pending_jobs:\n",
    "                    maap.cancelJob(job_id)\n",
    "                break\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    # Process the results once all jobs are completed\n",
    "    succeeded_job_ids = [job_id for job_id in job_ids\n",
    "                         if job_status_for(job_id) == \"Succeeded\"]\n",
    "    \n",
    "    failed_job_ids = [job_id for job_id in job_ids\n",
    "                      if job_status_for(job_id) == \"Failed\"]\n",
    "\n",
    "    other_job_ids = [job_id for job_id in job_ids\n",
    "                     if job_status_for(job_id)\n",
    "                     not in [\"Succeeded\", \"Failed\"]]\n",
    "\n",
    "    click.echo(f\"Processing results for {len(succeeded_job_ids)} \"\n",
    "               f\"succeeded jobs.\")\n",
    "\n",
    "    click.echo(f\"Gathering tarball paths from succeeded jobs.\")\n",
    "\n",
    "    tar_paths = []\n",
    "    for job_id in tqdm(succeeded_job_ids):\n",
    "        job_result_url = job_result_for(job_id)\n",
    "        time.sleep(1) # to avoid overwhelming the API\n",
    "        job_output_dir = to_job_output_dir(job_result_url, username)\n",
    "        # Find .tar.gz file in the output dir\n",
    "        tar_file = [f for f in os.listdir(job_output_dir)\n",
    "                     if f.endswith('.tar.gz')]\n",
    "        if len(tar_file) > 1:\n",
    "            warnings.warn(f\"Multiple .tar.gz files found in \"\n",
    "                          f\"{job_output_dir}.\")\n",
    "        if len(tar_file) == 0:\n",
    "            warnings.warn(f\"No .tar.gz files found in \"\n",
    "                          f\"{job_output_dir}.\")\n",
    "        if tar_file:\n",
    "            tar_paths.append(os.path.join(job_output_dir, tar_file[0]))\n",
    "\n",
    "    # Log the succeeded and failed job IDs\n",
    "    logging.info(f\"{len(succeeded_job_ids)} jobs succeeded.\")\n",
    "    logging.info(f\"Succeeded job IDs: {succeeded_job_ids}\\n\")\n",
    "    logging.info(f\"{len(failed_job_ids)} jobs failed.\")\n",
    "    logging.info(f\"Failed job IDs: {failed_job_ids}\\n\")\n",
    "    logging.info(f\"{len(other_job_ids)} jobs in other states.\")\n",
    "    logging.info(f\"Other job IDs: {other_job_ids}\\n\")\n",
    "\n",
    "    # Copy all tarballs to the output directory\n",
    "    click.echo(f\"Copying {len(tar_paths)} Tarballs to {output_dir}.\")\n",
    "    copy_batch_count = 0\n",
    "    for tar_path in tqdm(tar_paths):\n",
    "        try:\n",
    "            shutil.copy(tar_path, output_dir)\n",
    "            copy_batch_count += 1\n",
    "            if copy_batch_count == 50:\n",
    "                time.sleep(60)\n",
    "                copy_batch_count = 0\n",
    "            else:\n",
    "                time.sleep(2)\n",
    "            \n",
    "        except Exception as e:\n",
    "            warnings.warn(f\"Error copying {tar_path} to {output_dir}: {str(e)}\")\n",
    "            click.echo(\"Retrying in 10 seconds.\")\n",
    "            time.sleep(10)\n",
    "            try:\n",
    "                shutil.copy(tar_path, output_dir)\n",
    "            except Exception as e:\n",
    "                click.echo(f\"Retry failed: {str(e)}\")\n",
    "                click.echo(f\"Skipping {tar_path}.\")\n",
    "                continue\n",
    "\n",
    "    # Compress the output directory\n",
    "    # click.echo(f\"Compressing output directory.\")\n",
    "    # shutil.make_archive(output_dir, 'zip', output_dir)\n",
    "    # click.echo(f\"Output directory compressed to {output_dir}.zip.\")\n",
    "\n",
    "    end_time = datetime.datetime.now()\n",
    "\n",
    "    log_and_print(f\"Model run completed at {end_time}.\")\n",
    "    \n",
    "# if __name__ == \"__main__\":\n",
    "#     main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4452e4b0-71cb-4593-b9df-3c6333ef6d31",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Process the results once all jobs are completed\n",
    "succeeded_job_ids = [job_id for job_id in job_ids\n",
    "                     if job_status_for(job_id) == \"Succeeded\"]\n",
    "\n",
    "failed_job_ids = [job_id for job_id in job_ids\n",
    "                  if job_status_for(job_id) == \"Failed\"]\n",
    "\n",
    "other_job_ids = [job_id for job_id in job_ids\n",
    "                 if job_status_for(job_id)\n",
    "                 not in [\"Succeeded\", \"Failed\"]]\n",
    "\n",
    "click.echo(f\"Processing results for {len(succeeded_job_ids)} \"\n",
    "           f\"succeeded jobs.\")\n",
    "\n",
    "click.echo(f\"Gathering tarball paths from succeeded jobs.\")\n",
    "\n",
    "tar_paths = []\n",
    "for job_id in tqdm(succeeded_job_ids):\n",
    "    job_result_url = job_result_for(job_id)\n",
    "    time.sleep(1) # to avoid overwhelming the API\n",
    "    job_output_dir = to_job_output_dir(job_result_url, username)\n",
    "    # Find .tar.gz file in the output dir\n",
    "    tar_file = [f for f in os.listdir(job_output_dir)\n",
    "                 if f.endswith('.tar.gz')]\n",
    "    if len(tar_file) > 1:\n",
    "        warnings.warn(f\"Multiple .tar.gz files found in \"\n",
    "                      f\"{job_output_dir}.\")\n",
    "    if len(tar_file) == 0:\n",
    "        warnings.warn(f\"No .tar.gz files found in \"\n",
    "                      f\"{job_output_dir}.\")\n",
    "    if tar_file:\n",
    "        tar_paths.append(os.path.join(job_output_dir, tar_file[0]))\n",
    "\n",
    "# Log the succeeded and failed job IDs\n",
    "logging.info(f\"{len(succeeded_job_ids)} jobs succeeded.\")\n",
    "logging.info(f\"Succeeded job IDs: {succeeded_job_ids}\\n\")\n",
    "logging.info(f\"{len(failed_job_ids)} jobs failed.\")\n",
    "logging.info(f\"Failed job IDs: {failed_job_ids}\\n\")\n",
    "logging.info(f\"{len(other_job_ids)} jobs in other states.\")\n",
    "logging.info(f\"Other job IDs: {other_job_ids}\\n\")\n",
    "\n",
    "# Copy all tarballs to the output directory\n",
    "click.echo(f\"Copying {len(tar_paths)} Tarballs to {output_dir}.\")\n",
    "copy_batch_count = 0\n",
    "for tar_path in tqdm(tar_paths):\n",
    "    try:\n",
    "        shutil.copy(tar_path, output_dir)\n",
    "        copy_batch_count += 1\n",
    "        if copy_batch_count == 50:\n",
    "            time.sleep(60)\n",
    "            copy_batch_count = 0\n",
    "        else:\n",
    "            time.sleep(2)\n",
    "        \n",
    "    except Exception as e:\n",
    "        warnings.warn(f\"Error copying {tar_path} to {output_dir}: {str(e)}\")\n",
    "        click.echo(\"Retrying in 10 seconds.\")\n",
    "        time.sleep(10)\n",
    "        try:\n",
    "            shutil.copy(tar_path, output_dir)\n",
    "        except Exception as e:\n",
    "            click.echo(f\"Retry failed: {str(e)}\")\n",
    "            click.echo(f\"Skipping {tar_path}.\")\n",
    "            continue\n",
    "\n",
    "# Compress the output directory\n",
    "# click.echo(f\"Compressing output directory.\")\n",
    "# shutil.make_archive(output_dir, 'zip', output_dir)\n",
    "# click.echo(f\"Output directory compressed to {output_dir}.zip.\")\n",
    "\n",
    "end_time = datetime.datetime.now()\n",
    "\n",
    "log_and_print(f\"Model run completed at {end_time}.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "546c4df4-dd01-41fd-9bdf-1301f986cf08",
   "metadata": {},
   "source": [
    "## Above, split all files in the directory by submitting a split job then gather the results into a single directory\n",
    "\n",
    "## Below, for each tarball (original tile), "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "b90cf7d3-4049-445a-97b6-788c3a9bd948",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'identifier': 'Mean Shift Split LAS',\n",
       " 'algo_id': 'MS-Step-1-Split',\n",
       " 'version': 'main',\n",
       " 'username': 'jclayton0',\n",
       " 'queue': 'maap-dps-worker-64gb',\n",
       " 'LAS': 's3://maap-ops-workspace/jclayton0/sq_km_tiles_norm/TEAK_large_146.las',\n",
       " 'Subplot_width': 25,\n",
       " 'Buffer_width': 10}"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jobkwargs = get_split_kwargs(url)\n",
    "\n",
    "jobkwargs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "89cb607f-5eae-40b6-bc37-0612538c8529",
   "metadata": {},
   "outputs": [],
   "source": [
    "# jobkwargs = get_split_kwargs(test_file)\n",
    "\n",
    "# jobkwargs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "8dc28d6d-3425-485a-b3d5-30282bbc3d52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'job_id': '454a0bc9-87fc-476f-aae6-5066d1b77830', 'status': 'success', 'machine_type': None, 'architecture': None, 'machine_memory_size': None, 'directory_size': None, 'operating_system': None, 'job_start_time': None, 'job_end_time': None, 'job_duration_seconds': None, 'cpu_usage': None, 'cache_usage': None, 'mem_usage': None, 'max_mem_usage': None, 'swap_usage': None, 'read_io_stats': None, 'write_io_stats': None, 'sync_io_stats': None, 'async_io_stats': None, 'total_io_stats': None, 'error_details': None, 'response_code': 200, 'outputs': []}"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "maap.submitJob(**jobkwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "abe8851c-b0bc-4720-b48d-1a6036b7950b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../my-private-bucket/dps_output/MS-Step-1-Split-Buffer-PC-fixed/main/Mean Shift Split LAS/2024/10/29/13/37/59/605257/norm_TEAK_047_lidar_2021_point_clouds.tar.gz']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_output_dir = \"../my-private-bucket/dps_output/MS-Step-1-Split-Buffer-PC-fixed/main/Mean Shift Split LAS/2024/10/29/13/37/59/605257/\"\n",
    "\n",
    "split_output_dir_s3 = \"s3://maap-ops-workspace/jclayton0/dps_output/MS-Step-1-Split-Buffer-PC-fixed/main/Mean Shift Split LAS/2024/10/29/13/37/59/605257/\"\n",
    "tarball = glob.glob(f\"{split_output_dir}/*.tar.gz\")\n",
    "\n",
    "tarball"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9e0f6b54-56ab-4fd6-ab47-133ea41b4b72",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Path to the tarball file and extraction directory\n",
    "#tarball_path = \"path/to/yourfile.tar.gz\"\n",
    "extract_path = f\"{split_output_dir}rds/\"\n",
    "\n",
    "#os.makedirs(extract_path, exist_ok=False)\n",
    "# Open the tarball file\n",
    "with tarfile.open(tarball[0], \"r:gz\") as tar:\n",
    "    # Extract all contents to the specified directory\n",
    "    tar.extractall(path=extract_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "be6b1a00-81f2-4f5c-a56a-81118fcb67af",
   "metadata": {},
   "outputs": [],
   "source": [
    "pcs = glob.glob(f\"{extract_path}/norm*\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cd4f38e-e51c-41cb-9162-51cc6776539f",
   "metadata": {},
   "source": [
    "## Change the above to exclude find.txt; also remove the generation of find.txt from Split, if not done already"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f66166d1-7b8b-4e12-9092-8b00e08e1849",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../my-private-bucket/dps_output/MS-Step-1-Split-Buffer-PC-fixed/main/Mean Shift Split LAS/2024/10/29/13/37/59/605257/rds/norm_TEAK_047_lidar_2021_pc_1',\n",
       " '../my-private-bucket/dps_output/MS-Step-1-Split-Buffer-PC-fixed/main/Mean Shift Split LAS/2024/10/29/13/37/59/605257/rds/norm_TEAK_047_lidar_2021_pc_2',\n",
       " '../my-private-bucket/dps_output/MS-Step-1-Split-Buffer-PC-fixed/main/Mean Shift Split LAS/2024/10/29/13/37/59/605257/rds/norm_TEAK_047_lidar_2021_pc_3',\n",
       " '../my-private-bucket/dps_output/MS-Step-1-Split-Buffer-PC-fixed/main/Mean Shift Split LAS/2024/10/29/13/37/59/605257/rds/norm_TEAK_047_lidar_2021_pc_4',\n",
       " '../my-private-bucket/dps_output/MS-Step-1-Split-Buffer-PC-fixed/main/Mean Shift Split LAS/2024/10/29/13/37/59/605257/rds/norm_TEAK_047_lidar_2021_pc_5',\n",
       " '../my-private-bucket/dps_output/MS-Step-1-Split-Buffer-PC-fixed/main/Mean Shift Split LAS/2024/10/29/13/37/59/605257/rds/norm_TEAK_047_lidar_2021_pc_6',\n",
       " '../my-private-bucket/dps_output/MS-Step-1-Split-Buffer-PC-fixed/main/Mean Shift Split LAS/2024/10/29/13/37/59/605257/rds/norm_TEAK_047_lidar_2021_pc_7',\n",
       " '../my-private-bucket/dps_output/MS-Step-1-Split-Buffer-PC-fixed/main/Mean Shift Split LAS/2024/10/29/13/37/59/605257/rds/norm_TEAK_047_lidar_2021_pc_8',\n",
       " '../my-private-bucket/dps_output/MS-Step-1-Split-Buffer-PC-fixed/main/Mean Shift Split LAS/2024/10/29/13/37/59/605257/rds/norm_TEAK_047_lidar_2021_pc_9']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pcs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b5a9ca2b-bd50-4183-9470-941c1a1e2d1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "jobResponses = list()\n",
    "for file in pcs:\n",
    "    s3url = local_url_to_s3(file)\n",
    "    jobargs = get_segment_kwargs(s3url)\n",
    "    response = maap.submitJob(**jobargs)\n",
    "    jobResponses.append(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "959d2217-cd6a-4e78-ae04-f64bc2bdf60f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ast import literal_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "77c58e2f-b490-4158-af92-62c5769214f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "resulturl = job_result_for(literal_eval(str(jobResponses[0]))['job_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "571c3b91-f7f1-4d18-bd09-f2409bfa37d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://maap-ops-workspace.s3-website-us-west-2.amazonaws.com/jclayton0/dps_output/MS-Step-2-Segment-v2/main/Mean Shift Split LAS/2024/11/11/20/42/30/628677'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resulturl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d1f09c91-3cee-4490-8248-4a6f71208d33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/projects/my-private-bucket/dps_output/MS-Step-2-Segment-v2/main/Mean Shift Split LAS/2024/11/11/20/42/30/628677/seg_norm_TEAK_047_lidar_2021_pc_1\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[29], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m job \u001b[38;5;129;01min\u001b[39;00m jobResponses:\n\u001b[1;32m      3\u001b[0m     resulturl \u001b[38;5;241m=\u001b[39m to_job_output_dir(job_result_for(literal_eval(\u001b[38;5;28mstr\u001b[39m(job))[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mjob_id\u001b[39m\u001b[38;5;124m'\u001b[39m]), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mjclayton0\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 4\u001b[0m     file \u001b[38;5;241m=\u001b[39m \u001b[43mglob\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mglob\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mresulturl\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/seg*\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;28mprint\u001b[39m(file)\n\u001b[1;32m      6\u001b[0m     tile_list\u001b[38;5;241m.\u001b[39mappend(file)\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "tile_list = list()\n",
    "for job in jobResponses:\n",
    "    resulturl = to_job_output_dir(job_result_for(literal_eval(str(job))['job_id']), \"jclayton0\")\n",
    "    file = glob.glob(f\"{resulturl}/seg*\")[0]\n",
    "    print(file)\n",
    "    tile_list.append(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c51f788c-db20-4fc8-aef5-d5e1b3b1add7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "making segmented tile output directory\n"
     ]
    }
   ],
   "source": [
    "seg_tile_output_dir = f\"{output_dir}/segmented_tiles\"\n",
    "os.makedirs(seg_tile_output_dir, exist_ok=False)\n",
    "log_and_print(\"making segmented tile output directory\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb1212da-03a0-41e4-99b1-888b059e43f5",
   "metadata": {},
   "source": [
    "## Seems that the .rds extension is not being put on these files as necessary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f0334504-757b-41e5-97d0-d242f8523f29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# isolate filenames to move them\n",
    "filename_list = list()\n",
    "\n",
    "for tile in tile_list:\n",
    "    file = str.split(tile, '/')[-1]\n",
    "    filename_list.append(file)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7a800f49-2414-4f6b-a0cc-57f09c77b931",
   "metadata": {},
   "outputs": [],
   "source": [
    "# move all files to the folder we need them in\n",
    "for index in range(len(tile_list)):\n",
    "    old_path = tile_list[index]\n",
    "    new_path = f\"{seg_tile_output_dir}/{filename_list[index]}\"\n",
    "    os.rename(old_path, new_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ed5bac14-377d-485b-854b-f1d667b3eff6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compressed directory '/projects/my-private-bucket/run_output_20241111_203526/segmented_tiles' into '/projects/my-private-bucket/run_output_20241111_203526/segmented_tiles/TEAK_000_segmented.tar.gz'.\n"
     ]
    }
   ],
   "source": [
    "# tarball them to submit to job\n",
    "\n",
    "# Directory to compress and name of the output tarball\n",
    "output_tarball = f\"{seg_tile_output_dir}/TEAK_000_segmented.tar.gz\"\n",
    "\n",
    "# Create the tarball\n",
    "with tarfile.open(output_tarball, \"w:gz\") as tar:\n",
    "    # Add files to the tarball\n",
    "    for root, _, files in os.walk(seg_tile_output_dir):\n",
    "        for file in files:\n",
    "            file_path = os.path.join(root, file)\n",
    "            tar.add(file_path, arcname=os.path.relpath(file_path, seg_tile_output_dir))\n",
    "\n",
    "print(f\"Compressed directory '{seg_tile_output_dir}' into '{output_tarball}'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "878699a4-ca88-4c8b-8da9-42e7f97604f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'s3://maap-ops-workspace/jclayton0/run_output_20241111_203526/segmented_tiles/TEAK_000_segmented.tar.gz'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#tarball_url = local_url_to_s3(output_tarball)\n",
    "tarball_url = \"s3://maap-ops-workspace/jclayton0/run_output_20241111_203526/segmented_tiles/TEAK_000_segmented.tar.gz\"\n",
    "tarball_url "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "038f73f1-f5e3-4012-a4c9-ee304d57914e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'s3://maap-ops-workspace/jclayton0/normalized/norm_TEAK_047_lidar_2021.las'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "37e78d02-3d45-44ee-b4e8-29a4a828c07e",
   "metadata": {},
   "outputs": [],
   "source": [
    "reconcile_kwargs = get_reconcile_kwargs(tarball_url, test_file)\n",
    "rec_response = maap.submitJob(**reconcile_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1c95460-dffb-4022-9a62-dcb121d04d1a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
